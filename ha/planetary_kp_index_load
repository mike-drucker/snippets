#!/usr/bin/env python
import json
import os
import csv
from datetime import datetime
from requests import post, get
from local_token import HA_TOKEN

def LAST_COMPLETED_FILENAME():
    return 'noaa-planetary-k-index-forecast_completed.csv'

def DELTA_FILENAME():
    return '../noaa-planetary-k-index-forecast_delta.csv'

def download_data():
    url = "https://services.swpc.noaa.gov/products/noaa-planetary-k-index-forecast.json"
    response = get(url)
    if response.status_code != 200:
        raise Exception("Failed to Download Data. Status Code: " + str(response.status_code) + " Response Body: " + response.text)

    # Parse the JSON data
    data = json.loads(response.text)
    columns = data[0]  # Extract column names from the first row
    rows = data[1:]    # The rest are data rows

    # Convert rows to a list of dictionaries
    data_dicts = [dict(zip(columns, row)) for row in rows]

    # Process and structure the data
    for row in data_dicts:
        row["statistic_id"] = "sensor:planetary_kp_index"
        row["unit"] = "kp"
        row["start"] = datetime.strptime(row["time_tag"], "%Y-%m-%d %H:%M:%S")
        kp_value = float(row["kp"])
        row["min"] = kp_value
        row["max"] = kp_value
        row["mean"] = kp_value

    return data_dicts

def run_import():
    url = "http://localhost:8123/api/services/import_statistics/import_from_file"
    headers = {"Authorization": "Bearer " + HA_TOKEN()}
    data = {
        "timezone_identifier": "GMT",
        "delimiter": "\t",
        "decimal": False,
        "filename": os.path.basename(DELTA_FILENAME())
    }
    response = post(url, headers=headers, json=data)
    if response.status_code != 200:
        raise Exception("Failed to Import Data. Status Code: " + str(response.status_code) + " Response Body: " + response.text)

def get_existing():
    if os.path.isfile(LAST_COMPLETED_FILENAME()):
        existing_data = []
        with open(LAST_COMPLETED_FILENAME(), 'r', newline='') as csvfile:
            reader = csv.DictReader(csvfile, delimiter='\t')
            for row in reader:
                # Convert fields to appropriate data types
                row['start'] = datetime.strptime(row['start'], '%d.%m.%Y %H:%M')
                row['min'] = float(row['min'])
                row['max'] = float(row['max'])
                row['mean'] = float(row['mean'])
                existing_data.append(row)
        return existing_data
    else:
        return []

def get_delta_data(forecast_data, existing_data):
    existing_starts = set(row['start'] for row in existing_data)
    delta_data = [row for row in forecast_data if row['start'] not in existing_starts]
    print('DELTA:')
    print(delta_data)
    return delta_data

def save_delta(delta_data):
    with open(DELTA_FILENAME(), 'w', newline='') as csvfile:
        fieldnames = ['statistic_id', 'unit', 'start', 'min', 'max', 'mean']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter='\t')
        writer.writeheader()
        for row in delta_data:
            # Format 'start' field for output
            row['start'] = row['start'].strftime('%d.%m.%Y %H:%M')
            writer.writerow({field: row[field] for field in fieldnames})

def update_existing(forecast_data):
    with open(LAST_COMPLETED_FILENAME(), 'w', newline='') as csvfile:
        fieldnames = ['statistic_id', 'unit', 'start', 'min', 'max', 'mean']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter='\t')
        writer.writeheader()
        for row in forecast_data:
            # Format 'start' field for output
            row['start'] = row['start'].strftime('%d.%m.%Y %H:%M')
            writer.writerow({field: row[field] for field in fieldnames})

if __name__ == '__main__':
    forecast_data = download_data()
    existing_data = get_existing()
    delta_data = get_delta_data(forecast_data, existing_data)
    save_delta(delta_data)
    run_import()
    update_existing(forecast_data)
